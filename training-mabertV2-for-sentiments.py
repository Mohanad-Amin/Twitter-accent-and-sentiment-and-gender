# === Cell 1: Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆØ§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ ===
!pip install -q datasets evaluate

from google.colab import drive
drive.mount('/content/drive')

# Ù‚Ø§Ø¦Ù…Ø© Ù…Ø³Ø§Ø±Ø§Øª Ù…Ù„ÙØ§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
DATA_PATHS = [
    "/content/drive/MyDrive/Tweet_Project/Data/Arabic Sentiment Analysis 1_labelled.xlsx",
    "/content/drive/MyDrive/Tweet_Project/Data/full_tweets_clean2.xlsx",
    #"/content/drive/MyDrive/Tweet_Project/Data/Arabic Sentiment Analysis 3_labelled.xlsx",
]

OUT_DIR = "/content/drive/MyDrive/Tweet_Project/Models/sentiment_marbert_v308_improved"
MODEL_NAME = "UBC-NLP/MARBERTv2"

# ØªØ­Ø³ÙŠÙ† MAX_LEN Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ (99% Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ Ø£Ù‚Ù„ Ù…Ù† 99 Ø±Ù…Ø²)
MAX_LEN = 100  # Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† 128 - Ø³ÙŠÙˆÙØ± Ø°Ø§ÙƒØ±Ø© ÙˆÙˆÙ‚Øª
SEED = 42

import os, re, json, random
import numpy as np
import torch
os.makedirs(OUT_DIR, exist_ok=True)

# Ø¶Ø¨Ø· Ø§Ù„Ø¨Ø°ÙˆØ± Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙƒØ±Ø§Ø±
def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

set_seed(SEED)

import pandas as pd
import json
from tqdm import tqdm

def read_single_file(file_path):
    """Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù ÙˆØ§Ø­Ø¯ ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù…ÙˆØ¯ÙŠ text Ùˆ sentiment ÙÙ‚Ø·"""
    print(f"ğŸ“ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {file_path}")

    if not os.path.exists(file_path):
        print(f"âš ï¸ ØªØ­Ø°ÙŠØ±: Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {file_path}")
        return pd.DataFrame()

    try:
        df = pd.read_excel(file_path)
        print(f"   Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {df.shape}")
        print(f"   Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©: {df.columns.tolist()}")

        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¹Ù…ÙˆØ¯ÙŠÙ† Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ÙŠÙ†
        if "text" not in df.columns:
            print(f"âŒ Ø®Ø·Ø£: Ø¹Ù…ÙˆØ¯ 'text' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ {file_path}")
            return pd.DataFrame()

        if "sentiment" not in df.columns:
            print(f"âŒ Ø®Ø·Ø£: Ø¹Ù…ÙˆØ¯ 'sentiment' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ {file_path}")
            return pd.DataFrame()

        # Ø£Ø®Ø° Ø§Ù„Ø¹Ù…ÙˆØ¯ÙŠÙ† Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ÙŠÙ† ÙÙ‚Ø·
        df = df[["text", "sentiment"]].copy()

        # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØµØ¯Ø±
        df["source_file"] = os.path.basename(file_path)

        # ØªÙ†Ø¸ÙŠÙ Ø£ÙˆÙ„ÙŠ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        initial_count = len(df)
        df = df.dropna(subset=["text", "sentiment"])
        df = df[df["text"].astype(str).str.strip() != ""]
        df = df[df["sentiment"].astype(str).str.strip() != ""]

        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹ (Ø£Ù‚Ù„ Ù…Ù† 10 Ø£Ø­Ø±Ù)
        df = df[df["text"].str.len() >= 10]

        final_count = len(df)

        print(f"   ğŸ“Š Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµØ§Ù„Ø­Ø©: {final_count:,}/{initial_count:,}")

        if len(df) > 0:
            sample_text = df["text"].iloc[0]
            sample_sentiment = df["sentiment"].iloc[0]
            print(f"   ğŸ“ Ø¹ÙŠÙ†Ø©: {sample_text[:50]}... -> {sample_sentiment}")
            print(f"   âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­")
        else:
            print(f"   âš ï¸ ØªØ­Ø°ÙŠØ±: Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ØµØ§Ù„Ø­Ø© ÙÙŠ Ø§Ù„Ù…Ù„Ù")

        return df

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù {file_path}: {e}")
        return pd.DataFrame()

# Ù‚Ø±Ø§Ø¡Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª ÙˆØ¯Ù…Ø¬Ù‡Ø§
all_dataframes = []
total_original_rows = 0
file_stats = []

print("ğŸ”„ Ø¨Ø¯Ø¡ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„ÙØ§Øª...")
print("=" * 60)

for i, file_path in enumerate(DATA_PATHS, 1):
    print(f"\n[{i}/{len(DATA_PATHS)}] Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù:")
    df_single = read_single_file(file_path)

    if not df_single.empty:
        all_dataframes.append(df_single)
        total_original_rows += len(df_single)

        file_stat = {
            "file_path": file_path,
            "file_name": os.path.basename(file_path),
            "rows_count": len(df_single),
            "has_source_column": "source_file" in df_single.columns
        }

        file_stats.append(file_stat)

        print(f"   ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø·Ø±: {len(df_single):,}")

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¨ÙŠØ§Ù†Ø§Øª
if not all_dataframes:
    raise ValueError("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ø¨ÙŠØ§Ù†Ø§Øª ØµØ§Ù„Ø­Ø© ÙÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©")

# Ø¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
print(f"\nğŸ“‹ Ø¯Ù…Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† {len(all_dataframes)} Ù…Ù„Ù(Ø§Øª)...")
df = pd.concat(all_dataframes, ignore_index=True)

print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø³Ø·Ø± Ù‚Ø¨Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: {total_original_rows:,}")
print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø³Ø·Ø± Ø¨Ø¹Ø¯ Ø§Ù„Ø¯Ù…Ø¬: {len(df):,}")

# === Cell 2: ØªÙ†Ø¸ÙŠÙ ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ===

# ØªÙ†Ø¸ÙŠÙ Ù…Ø­Ø³Ù‘Ù† Ù„Ù„Ù†ØµÙˆØµ Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨ÙŠØ±ÙŠØ©
def enhanced_sentiment_clean(text):
    if not isinstance(text, str):
        return ""

    # Ø­ÙØ¸ Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠØ² Ù‚Ø¨Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # emoticons
        "\U0001F300-\U0001F5FF"  # symbols & pictographs
        "\U0001F680-\U0001F6FF"  # transport & map symbols
        "\U0001F1E0-\U0001F1FF"  # flags
        "\U00002702-\U000027B0"
        "\U000024C2-\U0001F251"
        "]+",
        flags=re.UNICODE
    )
    emojis = emoji_pattern.findall(text)

    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙˆØ§Ù„Ù…Ù†Ø´Ù†
    text = re.sub(r"http\S+|www\.\S+", " ", text)
    text = re.sub(r"@[A-Za-z0-9_]+", " ", text)
    text = re.sub(r"#(\w+)", r" \1 ", text)  # Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù‡Ø§Ø´ØªØ§Ø¬

    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ·ÙˆÙŠÙ„ Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ù…Ø¹Ø¨Ø± (Ù…Ø«Ù„ !!! Ø£Ùˆ ...)
    text = re.sub(r"([Ø£-ÙŠ])\1{2,}", r"\1\1", text)  # ØªØ·ÙˆÙŠÙ„ Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    text = re.sub(r"([!?.])\1{3,}", r"\1\1\1", text)  # Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ 3 ØªÙƒØ±Ø§Ø±Ø§Øª ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰

    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª
    text = re.sub(r"\s+", " ", text).strip()

    # Ø¥Ø¹Ø§Ø¯Ø© Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠØ² ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
    if emojis:
        text = text + " " + " ".join(emojis)

    return text

df["text"] = df["text"].apply(enhanced_sentiment_clean)

# Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„ÙØ§Ø±ØºØ© ÙˆØ§Ù„Ù…ÙƒØ±Ø±Ø©
print("ğŸ§¹ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ù…Ø¬Ø©...")

before_cleaning = len(df)
df = df.dropna(subset=["text","sentiment"])
df = df[df["text"].str.len() >= 10]  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
df = df.drop_duplicates(subset=["text"], keep="first")
df = df.reset_index(drop=True)

print(f"   Ù‚Ø¨Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: {before_cleaning:,} Ø³Ø·Ø±")
print(f"   Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: {len(df):,} Ø³Ø·Ø±")
print(f"   Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø­Ø°ÙˆÙ: {before_cleaning - len(df):,} Ø³Ø·Ø±")

# ØªØ­ÙˆÙŠÙ„ ØªØ³Ù…ÙŠØ§Øª Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù… - Ù…Ø­Ø³Ù‘Ù†
sentiment_mapping = {
    # Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    "positive": 2, "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ": 2, "Ø§ÙŠØ¬Ø§Ø¨ÙŠ": 2, "Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©": 2, "Ø§ÙŠØ¬Ø§Ø¨ÙŠØ©": 2,
    "negative": 0, "Ø³Ù„Ø¨ÙŠ": 0, "Ø³Ø§Ù„Ø¨": 0, "Ø³Ù„Ø¨ÙŠØ©": 0, "Ø³Ø§Ù„Ø¨Ø©": 0,
    "neutral": 1, "Ù…Ø­Ø§ÙŠØ¯": 1, "Ù…ØªÙˆØ³Ø·": 1, "Ù…Ø­Ø§ÙŠØ¯Ø©": 1,
    # Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
    "pos": 2, "neg": 0, "neu": 1,
    # Ø§Ù„Ø£Ø±Ù‚Ø§Ù…
    1: 2, 2: 2,     # Ø¥ÙŠØ¬Ø§Ø¨ÙŠ
    -1: 0, 0: 0,    # Ø³Ù„Ø¨ÙŠ
}

def map_sentiment(sentiment):
    if isinstance(sentiment, str):
        sentiment = sentiment.strip().lower()

    if sentiment in sentiment_mapping:
        return sentiment_mapping[sentiment]

    # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø±Ù‚Ù…
    try:
        val = float(sentiment)
        if val > 0:
            return 2  # Ø¥ÙŠØ¬Ø§Ø¨ÙŠ
        elif val < 0:
            return 0  # Ø³Ù„Ø¨ÙŠ
        else:
            return 1  # Ù…Ø­Ø§ÙŠØ¯
    except:
        pass

    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
    sentiment_str = str(sentiment).lower()
    if any(word in sentiment_str for word in ["positive", "pos", "Ø¥ÙŠØ¬Ø§Ø¨", "Ø§ÙŠØ¬Ø§Ø¨", "Ø¬ÙŠØ¯", "Ù…Ù…ØªØ§Ø²", "Ø±Ø§Ø¦Ø¹"]):
        return 2
    elif any(word in sentiment_str for word in ["negative", "neg", "Ø³Ù„Ø¨", "Ø³ÙŠØ¡", "ÙØ§Ø³Ø¯", "Ø³ÙˆØ¡"]):
        return 0
    else:
        return 1  # Ù…Ø­Ø§ÙŠØ¯ ÙƒÙ‚ÙŠÙ…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©

df["labels"] = df["sentiment"].apply(map_sentiment)
df = df.dropna(subset=["labels"]).reset_index(drop=True)
df["labels"] = df["labels"].astype(int)

# Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù‚ÙŠÙ… Ø¶Ù…Ù† Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹
valid_labels = set([0, 1, 2])
seen = set(df["labels"].unique().tolist())
assert seen.issubset(valid_labels), f"ÙˆØ¬Ø¯Øª Ù‚ÙŠÙ… Ù„ÙŠØ¨Ù„ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹Ø©: {sorted(seen - valid_labels)}"

# Ø®Ø±Ø§Ø¦Ø· ÙØ¦Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
id2label = {
    0: "Ø³Ù„Ø¨ÙŠ",      # Negative
    1: "Ù…Ø­Ø§ÙŠØ¯",     # Neutral
    2: "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ"     # Positive
}
label2id = {v:k for k,v in id2label.items()}

print("\nğŸ“Š ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±:")
sentiment_counts = df["labels"].value_counts().sort_index()
for idx, count in sentiment_counts.items():
    print(f"{id2label[idx]}: {count:,} ({count/len(df)*100:.1f}%)")

# === Cell 3: Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ø¯Ù… Ø§Ù„ØªÙˆØ§Ø²Ù† ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ===

# Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… undersampling Ù„Ù„ÙØ¦Ø© Ø§Ù„Ø£ÙƒØ«Ø±
from sklearn.utils import resample

print("\nâš–ï¸ Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")

# Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ù‚Ù„ Ø¹Ø¯Ø¯ ÙÙŠ Ø§Ù„ÙØ¦Ø§Øª
min_class_count = sentiment_counts.min()
target_count = int(min_class_count * 1.1)  # Ù†Ø£Ø®Ø° 110% Ù…Ù† Ø£Ù‚Ù„ ÙØ¦Ø©

balanced_dfs = []
for label in [0, 1, 2]:
    df_class = df[df['labels'] == label]
    if len(df_class) > target_count:
        # Undersample
        df_class_sampled = resample(df_class,
                                   n_samples=target_count,
                                   random_state=SEED)
    else:
        # Oversample Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ÙØ¦Ø© Ø£Ù‚Ù„ Ù…Ù† Ø§Ù„Ù‡Ø¯Ù
        df_class_sampled = resample(df_class,
                                   n_samples=target_count,
                                   replace=True,
                                   random_state=SEED)
    balanced_dfs.append(df_class_sampled)

df_balanced = pd.concat(balanced_dfs)
df_balanced = df_balanced.sample(frac=1, random_state=SEED).reset_index(drop=True)

print(f"Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„Ù…ÙˆØ§Ø²Ù†Ø©: {len(df_balanced):,} Ø¹ÙŠÙ†Ø©")
print("Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¬Ø¯ÙŠØ¯:")
for label, count in df_balanced['labels'].value_counts().sort_index().items():
    print(f"  {id2label[label]}: {count:,} ({count/len(df_balanced)*100:.1f}%)")

# Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆØ§Ø²Ù†Ø©
df = df_balanced

# === Cell 4: Ø§Ù„ØªÙ‚Ø³ÙŠÙ… ÙˆØ§Ù„ØªØ±Ù…ÙŠØ² ===
from sklearn.model_selection import train_test_split
from datasets import Dataset, DatasetDict

# ØªÙ‚Ø³ÙŠÙ… stratified 70/15/15 (Ø£ÙØ¶Ù„ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆØ§Ø²Ù†Ø©)
train_columns = ["text", "labels"]

train_df, temp_df = train_test_split(
    df[train_columns], test_size=0.3, stratify=df["labels"], random_state=SEED
)
val_df, test_df = train_test_split(
    temp_df, test_size=0.5, stratify=temp_df["labels"], random_state=SEED
)

print(f"\nğŸ“Š ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
print(f"Train: {len(train_df):,} ({len(train_df)/len(df)*100:.1f}%)")
print(f"Val: {len(val_df):,} ({len(val_df)/len(df)*100:.1f}%)")
print(f"Test: {len(test_df):,} ({len(test_df)/len(df)*100:.1f}%)")

ds = DatasetDict({
    "train": Dataset.from_pandas(train_df.reset_index(drop=True)),
    "validation": Dataset.from_pandas(val_df.reset_index(drop=True)),
    "test": Dataset.from_pandas(test_df.reset_index(drop=True)),
})

# === Cell 5: Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø­Ø³Ù‘Ù† ===
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    EarlyStoppingCallback,
    DataCollatorWithPadding
)
import evaluate

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙˆÙƒÙŠÙ†Ø§ÙŠØ²Ø± ÙˆØ§Ù„Ù†Ù…ÙˆØ°Ø¬
tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
num_labels = len(id2label)

model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=num_labels,
    id2label=id2label,
    label2id=label2id,
    hidden_dropout_prob=0.2,  # Ø¥Ø¶Ø§ÙØ© dropout Ù„Ù„ØªØ¹Ù…ÙŠÙ…
    attention_probs_dropout_prob=0.2
)

# Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©
def tokenize(batch):
    return tok(
        batch["text"],
        truncation=True,
        max_length=MAX_LEN,
        padding=False  # Ø³Ù†Ø³ØªØ®Ø¯Ù… dynamic padding
    )

# ØªØ±Ù…ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
print("ğŸ”„ ØªØ±Ù…ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
ds_enc = ds.map(tokenize, batched=True, remove_columns=["text"])

# Data collator Ù„Ù„Ù€ dynamic padding
data_collator = DataCollatorWithPadding(tokenizer=tok, padding=True)

# Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©
f1_metric = evaluate.load("f1")
acc_metric = evaluate.load("accuracy")
precision_metric = evaluate.load("precision")
recall_metric = evaluate.load("recall")

def compute_metrics(eval_pred):
    logits, labels_np = eval_pred
    preds = logits.argmax(axis=-1)

    results = {}
    results["accuracy"] = acc_metric.compute(predictions=preds, references=labels_np)["accuracy"]
    results["macro_f1"] = f1_metric.compute(predictions=preds, references=labels_np, average="macro")["f1"]
    results["weighted_f1"] = f1_metric.compute(predictions=preds, references=labels_np, average="weighted")["f1"]

    # F1 Ù„ÙƒÙ„ ÙØ¦Ø©
    for i, label_name in id2label.items():
        label_f1 = f1_metric.compute(
            predictions=preds,
            references=labels_np,
            labels=[i],
            average="micro"
        )["f1"]
        results[f"f1_{label_name}"] = label_f1

    results["macro_precision"] = precision_metric.compute(predictions=preds, references=labels_np, average="macro")["precision"]
    results["macro_recall"] = recall_metric.compute(predictions=preds, references=labels_np, average="macro")["recall"]

    return results

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©
training_args = TrainingArguments(
    output_dir=OUT_DIR,
    eval_strategy="steps",
    eval_steps=200,  # ØªÙ‚ÙŠÙŠÙ… Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹
    save_strategy="steps",
    save_steps=200,
    save_total_limit=3,
    learning_rate=2e-5,  # Ù…Ø¹Ø¯Ù„ ØªØ¹Ù„Ù… Ø£Ù‚Ù„ Ù„Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±
    per_device_train_batch_size=32,  # batch size Ù…ØªÙˆØ³Ø·
    per_device_eval_batch_size=64,
    num_train_epochs=5,  # epochs Ù…Ø¹ØªØ¯Ù„ Ù…Ø¹ early stopping
    weight_decay=0.01,
    warmup_ratio=0.1,
    logging_steps=50,
    load_best_model_at_end=True,
    metric_for_best_model="macro_f1",
    greater_is_better=True,
    fp16=True,  # Ø§Ø³ØªØ®Ø¯Ø§Ù… FP16 Ù„Ù„Ø³Ø±Ø¹Ø©
    gradient_checkpointing=True,
    gradient_accumulation_steps=2,  # Ù„Ù…Ø­Ø§ÙƒØ§Ø© batch size Ø£ÙƒØ¨Ø±
    dataloader_drop_last=False,
    eval_accumulation_steps=1,
    push_to_hub=False,
    report_to="none",  # ØªØ¹Ø·ÙŠÙ„ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø©
    optim="adamw_torch",  # Ù…Ø­Ø³Ù‘Ù† Ø£ÙØ¶Ù„
    lr_scheduler_type="cosine",  # Ø¬Ø¯ÙˆÙ„Ø© Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
    seed=SEED,
    data_seed=SEED,
    label_smoothing_factor=0.1,  # ØªÙ†Ø¹ÙŠÙ… Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ù„Ù„ØªØ¹Ù…ÙŠÙ…
)

# Ø§Ù„Ù€ Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=ds_enc["train"],
    eval_dataset=ds_enc["validation"],
    tokenizer=tok,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callbacks=[
        EarlyStoppingCallback(
            early_stopping_patience=3,
            early_stopping_threshold=0.001
        )
    ]
)

# Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
print("ğŸš€ Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…Ø­Ø³Ù‘Ù†...")
train_result = trainer.train()

# Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
print("ğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
trainer.save_model(OUT_DIR)
tok.save_pretrained(OUT_DIR)

# Ø­ÙØ¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
training_info = {
    "id2label": id2label,
    "label2id": label2id,
    "num_labels": num_labels,
    "task": "sentiment_analysis",
    "language": "arabic",
    "model_name": MODEL_NAME,
    "max_length": MAX_LEN,
    "training_samples": len(train_df),
    "validation_samples": len(val_df),
    "test_samples": len(test_df),
    "train_runtime": train_result.metrics["train_runtime"],
    "train_samples_per_second": train_result.metrics["train_samples_per_second"],
    "train_loss": train_result.metrics["train_loss"],
    "data_info": {
        "original_samples": total_original_rows,
        "cleaned_samples": before_cleaning,
        "balanced_samples": len(df),
        "avg_text_length": df["text"].str.len().mean(),
        "sentiment_distribution": df["labels"].value_counts().to_dict()
    }
}

with open(os.path.join(OUT_DIR, "training_info.json"), "w", encoding="utf-8") as f:
    json.dump(training_info, f, ensure_ascii=False, indent=2)

print("âœ… ØªÙ… Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…Ø­Ø³Ù‘Ù† ÙÙŠ:", OUT_DIR)

# === Cell 6: Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø´Ø§Ù…Ù„ ===
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ
import matplotlib.font_manager as fm
# Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥ÙŠØ¬Ø§Ø¯ Ø®Ø· Ø¹Ø±Ø¨ÙŠ
arabic_fonts = [f for f in fm.findSystemFonts() if 'arabic' in f.lower() or 'arial' in f.lower()]
if arabic_fonts:
    plt.rcParams['font.family'] = fm.FontProperties(fname=arabic_fonts[0]).get_name()

# ØªÙ‚ÙŠÙŠÙ… Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
print("\nğŸ” ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±...")
pred = trainer.predict(ds_enc["test"])
preds = np.argmax(pred.predictions, axis=-1)
true = pred.label_ids

# Ø£Ø³Ù…Ø§Ø¡ ÙØ¦Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
target_names = [id2label[i] for i in range(num_labels)]

# ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…ÙØµÙ„
report = classification_report(true, preds, target_names=target_names, digits=4)
print("\nğŸ“Š ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±:")
print("=" * 50)
print(report)

# ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„ Ø¨ØµÙŠØºØ© dictionary
report_dict = classification_report(true, preds, target_names=target_names, output_dict=True)

# Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
test_metrics = {
    "test_accuracy": float(acc_metric.compute(predictions=preds, references=true)["accuracy"]),
    "test_macro_f1": float(f1_metric.compute(predictions=preds, references=true, average="macro")["f1"]),
    "test_weighted_f1": float(f1_metric.compute(predictions=preds, references=true, average="weighted")["f1"]),
    "test_macro_precision": float(precision_metric.compute(predictions=preds, references=true, average="macro")["precision"]),
    "test_macro_recall": float(recall_metric.compute(predictions=preds, references=true, average="macro")["recall"]),
    "n_test_samples": int(len(true)),
    "per_class_metrics": {
        label_name: {
            "precision": report_dict[label_name]["precision"],
            "recall": report_dict[label_name]["recall"],
            "f1-score": report_dict[label_name]["f1-score"],
            "support": report_dict[label_name]["support"]
        }
        for label_name in target_names
    }
}

print("\nğŸ“ˆ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©:")
print("=" * 30)
for metric, value in test_metrics.items():
    if metric not in ["n_test_samples", "per_class_metrics"]:
        print(f"{metric}: {value:.4f}")
    elif metric == "n_test_samples":
        print(f"{metric}: {value}")

# Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙˆØ§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
with open(os.path.join(OUT_DIR, "evaluation_report.txt"), "w", encoding="utf-8") as f:
    f.write("ØªÙ‚Ø±ÙŠØ± ØªÙ‚ÙŠÙŠÙ… Ù†Ù…ÙˆØ°Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…Ø­Ø³Ù‘Ù†\n")
    f.write("=" * 50 + "\n\n")
    f.write(report + "\n\n")
    f.write("Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©:\n")
    f.write(json.dumps(test_metrics, ensure_ascii=False, indent=2))

# Ø±Ø³Ù… Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³ Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©
cm = confusion_matrix(true, preds, labels=list(range(num_labels)))

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=target_names, yticklabels=target_names,
            cbar_kws={'label': 'Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª'})
plt.title('Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³ - ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±', fontsize=16, pad=20)
plt.xlabel('Ø§Ù„ØªÙˆÙ‚Ø¹', fontsize=14)
plt.ylabel('Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø©', fontsize=14)

# Ø¥Ø¶Ø§ÙØ© Ù†Ø³Ø¨ Ù…Ø¦ÙˆÙŠØ©
for i in range(len(cm)):
    for j in range(len(cm)):
        percentage = cm[i, j] / cm[i].sum() * 100
        plt.text(j + 0.5, i + 0.7, f'{percentage:.1f}%',
                ha='center', va='center', fontsize=9, color='gray')

plt.tight_layout()
cm_path = os.path.join(OUT_DIR, "confusion_matrix.png")
plt.savefig(cm_path, dpi=300, bbox_inches='tight')
plt.show()

# Ø±Ø³Ù… Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù„ÙƒÙ„ ÙØ¦Ø©
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

# Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø£ÙˆÙ„: F1-Score Ù„ÙƒÙ„ ÙØ¦Ø©
f1_scores = [test_metrics["per_class_metrics"][label]["f1-score"] for label in target_names]
bars1 = ax1.bar(target_names, f1_scores, color=['red', 'gray', 'green'], alpha=0.7)
ax1.set_title('F1-Score Ù„ÙƒÙ„ ÙØ¦Ø©', fontsize=14)
ax1.set_ylabel('F1-Score', fontsize=12)
ax1.set_ylim(0, 1)
ax1.grid(axis='y', alpha=0.3)

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù‚ÙŠÙ… Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
for bar, score in zip(bars1, f1_scores):
    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
            f'{score:.3f}', ha='center', va='bottom')

# Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø«Ø§Ù†ÙŠ: Precision Ùˆ Recall
x = np.arange(len(target_names))
width = 0.35

precision_scores = [test_metrics["per_class_metrics"][label]["precision"] for label in target_names]
recall_scores = [test_metrics["per_class_metrics"][label]["recall"] for label in target_names]

bars2 = ax2.bar(x - width/2, precision_scores, width, label='Precision', alpha=0.7)
bars3 = ax2.bar(x + width/2, recall_scores, width, label='Recall', alpha=0.7)

ax2.set_title('Precision Ùˆ Recall Ù„ÙƒÙ„ ÙØ¦Ø©', fontsize=14)
ax2.set_ylabel('Ø§Ù„Ù‚ÙŠÙ…Ø©', fontsize=12)
ax2.set_xticks(x)
ax2.set_xticklabels(target_names)
ax2.legend()
ax2.set_ylim(0, 1)
ax2.grid(axis='y', alpha=0.3)

plt.tight_layout()
metrics_path = os.path.join(OUT_DIR, "performance_metrics.png")
plt.savefig(metrics_path, dpi=300, bbox_inches='tight')
plt.show()

# === Cell 7: Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ­ÙØ¸ Ø£Ù…Ø«Ù„Ø© ===
def test_sentiment_model(texts, model_path=OUT_DIR):
    """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù†ØµÙˆØµ Ø¬Ø¯ÙŠØ¯Ø©"""
    from transformers import pipeline

    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙƒÙ€ pipeline
    classifier = pipeline(
        "text-classification",
        model=model_path,
        tokenizer=model_path,
        device=0 if torch.cuda.is_available() else -1,
        max_length=MAX_LEN,
        truncation=True
    )

    results = []
    for text in texts:
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ§Øª
        result = classifier(text, top_k=None)

        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø­Ø³Ø¨ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©
        result_sorted = sorted(result, key=lambda x: x['score'], reverse=True)

        results.append({
            "text": text,
            "predicted_sentiment": result_sorted[0]["label"],
            "confidence": result_sorted[0]["score"],
            "all_scores": {r["label"]: r["score"] for r in result_sorted}
        })

    return results

# Ù†ØµÙˆØµ ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ù…ØªÙ†ÙˆØ¹Ø© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
test_texts = [
    "Ø£Ø­Ø¨ Ù‡Ø°Ø§ Ø§Ù„ÙÙŠÙ„Ù… ÙƒØ«ÙŠØ±Ø§Ù‹ØŒ Ø¥Ù†Ù‡ Ø±Ø§Ø¦Ø¹ Ø¬Ø¯Ø§Ù‹! ğŸ˜",
    "Ù‡Ø°Ø§ Ø§Ù„Ù…Ø·Ø¹Ù… Ø³ÙŠØ¡ Ù„Ù„ØºØ§ÙŠØ©ØŒ Ù„Ù† Ø£Ø¹ÙˆØ¯ Ø¥Ù„ÙŠÙ‡ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ ğŸ˜ ",
    "Ø§Ù„Ø·Ù‚Ø³ Ø§Ù„ÙŠÙˆÙ… Ø¹Ø§Ø¯ÙŠØŒ Ù„Ø§ Ù‡Ùˆ Ø­Ø§Ø± ÙˆÙ„Ø§ Ø¨Ø§Ø±Ø¯",
    "Ø´ÙƒØ±Ø§Ù‹ Ù„Ùƒ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©ØŒ Ø£Ù‚Ø¯Ø± Ø¬Ù‡ÙˆØ¯Ùƒ ÙƒØ«ÙŠØ±Ø§Ù‹ â¤ï¸",
    "Ù„Ø§ Ø£Ø¹Ø±Ù Ù…Ø§Ø°Ø§ Ø£Ù‚ÙˆÙ„ Ø¹Ù† Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹",
    "Ø§Ù„Ø®Ø¯Ù…Ø© ÙƒØ§Ù†Øª Ø¨Ø·ÙŠØ¦Ø© Ù„ÙƒÙ† Ø§Ù„Ø·Ø¹Ø§Ù… Ù„Ø°ÙŠØ°",
    "Ø£Ø³ÙˆØ£ ØªØ¬Ø±Ø¨Ø© ÙÙŠ Ø­ÙŠØ§ØªÙŠ! ğŸ˜¤",
    "Ù…Ù†ØªØ¬ Ø¹Ø§Ø¯ÙŠØŒ Ù„Ø§ ÙŠØ³ØªØ­Ù‚ Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ù…Ø¯ÙÙˆØ¹",
    "Ù…Ù…ØªØ§Ø²! ØªØ¬Ø§ÙˆØ² ÙƒÙ„ ØªÙˆÙ‚Ø¹Ø§ØªÙŠ ğŸŒŸ",
    "Ø§Ù„Ù…Ù†ØªØ¬ ÙˆØµÙ„ Ù…ØªØ£Ø®Ø±Ø§Ù‹ ÙˆÙ…Ø¹Ø·ÙˆØ¨Ø§Ù‹ØŒ Ø®Ø¯Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ù„Ù… ØªØ³Ø§Ø¹Ø¯"
]

print("\nğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù†ØµÙˆØµ Ø¬Ø¯ÙŠØ¯Ø©:")
print("=" * 80)
test_results = test_sentiment_model(test_texts)

# Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ø´ÙƒÙ„ Ù…ÙØµÙ„
for i, result in enumerate(test_results, 1):
    print(f"\n{i}. Ø§Ù„Ù†Øµ: {result['text']}")
    print(f"   ğŸ­ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©: {result['predicted_sentiment']}")
    print(f"   ğŸ“Š Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©: {result['confidence']:.3f}")
    print(f"   ğŸ“ˆ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ§Øª:")
    for sentiment, score in result['all_scores'].items():
        bar = "â–ˆ" * int(score * 20)
        print(f"      {sentiment}: {bar} {score:.3f}")

# Ø­ÙØ¸ Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
test_examples = {
    "test_date": pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S"),
    "model_path": OUT_DIR,
    "examples": test_results
}

with open(os.path.join(OUT_DIR, "test_examples.json"), "w", encoding="utf-8") as f:
    json.dump(test_examples, f, ensure_ascii=False, indent=2)

# === Cell 8: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ===
print("\nğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡...")

# Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
errors = []
for i, (pred_label, true_label) in enumerate(zip(preds, true)):
    if pred_label != true_label:
        text = test_df.iloc[i]["text"]
        errors.append({
            "text": text,
            "true_label": id2label[true_label],
            "pred_label": id2label[pred_label],
            "text_length": len(text)
        })

print(f"\nØ¹Ø¯Ø¯ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡: {len(errors)} Ù…Ù† Ø£ØµÙ„ {len(test_df)} ({len(errors)/len(test_df)*100:.1f}%)")

# ØªØ­Ù„ÙŠÙ„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
error_types = {}
for error in errors:
    error_type = f"{error['true_label']} â†’ {error['pred_label']}"
    if error_type not in error_types:
        error_types[error_type] = []
    error_types[error_type].append(error)

print("\nğŸ“Š ØªÙˆØ²ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡:")
for error_type, error_list in sorted(error_types.items(), key=lambda x: len(x[1]), reverse=True):
    print(f"   {error_type}: {len(error_list)} Ø®Ø·Ø£ ({len(error_list)/len(errors)*100:.1f}%)")

# Ø¹Ø±Ø¶ Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø£ÙƒØ«Ø± Ø´ÙŠÙˆØ¹Ø§Ù‹
print("\nğŸ“ Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡:")
for error_type, error_list in sorted(error_types.items(), key=lambda x: len(x[1]), reverse=True)[:3]:
    print(f"\n{error_type}:")
    for error in error_list[:2]:  # Ø¹Ø±Ø¶ Ù…Ø«Ø§Ù„ÙŠÙ† ÙÙ‚Ø·
        print(f"   - {error['text'][:80]}...")

# Ø­ÙØ¸ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
error_analysis = {
    "total_errors": len(errors),
    "error_rate": len(errors) / len(test_df),
    "error_types": {k: len(v) for k, v in error_types.items()},
    "sample_errors": errors[:20]  # Ø­ÙØ¸ 20 Ù…Ø«Ø§Ù„ ÙÙ‚Ø·
}

with open(os.path.join(OUT_DIR, "error_analysis.json"), "w", encoding="utf-8") as f:
    json.dump(error_analysis, f, ensure_ascii=False, indent=2)

# === Cell 9: Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ù†Ù‡Ø§Ø¦ÙŠ ===
print("\nğŸ“„ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ...")

final_report = f"""
# ØªÙ‚Ø±ÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©

## ğŸ“Š Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ø§Ù…Ø©
- **Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ**: {MODEL_NAME}
- **ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ¯Ø±ÙŠØ¨**: {pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S")}
- **Ù…Ø¯Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨**: {training_info['train_runtime']:.2f} Ø«Ø§Ù†ÙŠØ©
- **Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª**:
  - Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {training_info['training_samples']:,}
  - Ø§Ù„ØªØ­Ù‚Ù‚: {training_info['validation_samples']:,}
  - Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {training_info['test_samples']:,}

## ğŸ“ˆ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
- **Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©**: {test_metrics['test_accuracy']:.4f}
- **F1-Score (Macro)**: {test_metrics['test_macro_f1']:.4f}
- **F1-Score (Weighted)**: {test_metrics['test_weighted_f1']:.4f}

## ğŸ­ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø­Ø³Ø¨ Ø§Ù„ÙØ¦Ø©
"""

for label_name in target_names:
    metrics = test_metrics['per_class_metrics'][label_name]
    final_report += f"\n### {label_name}:\n"
    final_report += f"- Precision: {metrics['precision']:.4f}\n"
    final_report += f"- Recall: {metrics['recall']:.4f}\n"
    final_report += f"- F1-Score: {metrics['f1-score']:.4f}\n"
    final_report += f"- Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª: {metrics['support']}\n"

final_report += f"""
## ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
- **Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø®Ø·Ø£**: {error_analysis['error_rate']:.2%}
- **Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡**: {error_analysis['total_errors']}

### Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©:
"""

for error_type, count in sorted(error_analysis['error_types'].items(), key=lambda x: x[1], reverse=True)[:5]:
    percentage = count / error_analysis['total_errors'] * 100
    final_report += f"- {error_type}: {count} ({percentage:.1f}%)\n"

final_report += f"""
## ğŸ’¾ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©
1. **Ø§Ù„Ù†Ù…ÙˆØ°Ø¬**: `pytorch_model.bin`
2. **Tokenizer**: `tokenizer_config.json`, `special_tokens_map.json`, `vocab.txt`
3. **Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨**: `training_info.json`
4. **ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªÙ‚ÙŠÙŠÙ…**: `evaluation_report.txt`
5. **Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³**: `confusion_matrix.png`
6. **Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡**: `performance_metrics.png`
7. **Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±**: `test_examples.json`
8. **ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡**: `error_analysis.json`

## ğŸš€ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬

```python
from transformers import pipeline

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
classifier = pipeline(
    "text-classification",
    model="{OUT_DIR}",
    tokenizer="{OUT_DIR}"
)

# Ø§Ù„ØªÙ†Ø¨Ø¤
result = classifier("Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØ­Ù„ÙŠÙ„Ù‡")
print(result)
```

## ğŸ“ Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†
1. ØªÙ… Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙØ¦Ø§Øª
2. ØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª ØªÙ†Ø¸ÙŠÙ… (dropout, label smoothing) Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ¹Ù…ÙŠÙ…
3. ØªÙ… ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠØ² ÙˆØ§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨ÙŠØ±ÙŠØ©
4. ØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… dynamic padding Ù„ØªØ­Ø³ÙŠÙ† ÙƒÙØ§Ø¡Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
5. ØªÙ… ØªÙ‚Ù„ÙŠÙ„ MAX_LEN Ø¥Ù„Ù‰ 100 Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
"""

with open(os.path.join(OUT_DIR, "final_report.md"), "w", encoding="utf-8") as f:
    f.write(final_report)

print("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ")

# === Cell 10: Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ===
print("\n" + "="*80)
print("ğŸ‰ Ø§ÙƒØªÙ…Ù„ ØªØ¯Ø±ÙŠØ¨ ÙˆØªÙ‚ÙŠÙŠÙ… Ù†Ù…ÙˆØ°Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…Ø­Ø³Ù‘Ù† Ø¨Ù†Ø¬Ø§Ø­!")
print("="*80)

print(f"\nğŸ“Š Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬:")
print(f"   â€¢ Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: {test_metrics['test_accuracy']:.2%}")
print(f"   â€¢ F1-Score (Macro): {test_metrics['test_macro_f1']:.4f}")
print(f"   â€¢ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø®Ø·Ø£: {error_analysis['error_rate']:.2%}")

print(f"\nğŸ“ Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {OUT_DIR}")

print(f"\nğŸ’¡ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø·Ø¨Ù‚Ø©:")
print(f"   âœ“ Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ­ÙŠØ²")
print(f"   âœ“ ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©")
print(f"   âœ“ Ø§Ø³ØªØ®Ø¯Ø§Ù… dynamic padding")
print(f"   âœ“ ØªØ·Ø¨ÙŠÙ‚ ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©")
print(f"   âœ“ Ø¶Ø¨Ø· hyperparameters Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")

print(f"\nğŸ”„ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©:")
print(f"   1. ØªØ¬Ø±Ø¨Ø© Ù†Ù…Ø§Ø°Ø¬ Ø£Ø®Ø±Ù‰ (CAMeL-BERT, AraBERT)")
print(f"   2. ØªØ·Ø¨ÙŠÙ‚ Data Augmentation")
print(f"   3. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ensemble Methods")
print(f"   4. ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·ÙˆÙŠÙ„Ø©")

print("\nâœ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…!")
