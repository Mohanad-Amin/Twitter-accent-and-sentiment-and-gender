import os
import re
import time
import json
import math
import random
import requests
import pandas as pd
from collections import deque
from google.colab import userdata
from google.colab import drive

"""
Ù…ÙˆÙ„Ù‘ÙØ¯ ØªØºØ±ÙŠØ¯Ø§Øª Ù„Ù„Ù‡Ø¬Ø§Øª Ø§Ù„Ø´Ø§Ù…ÙŠØ© (Ø³ÙˆØ±ÙŠØ©/Ù„Ø¨Ù†Ø§Ù†ÙŠØ©/Ø£Ø±Ø¯Ù†ÙŠØ©/ÙÙ„Ø³Ø·ÙŠÙ†ÙŠØ©)
Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… GPT-4o Ø¹Ø¨Ø± **OpenAI Batch API ÙÙ‚Ø·**
(Ø§Ù„Ù†ØªÙŠØ¬Ø©: Ù…Ù„Ù Excel Ø¨Ø«Ù„Ø§Ø«Ø© Ø£Ø¹Ù…Ø¯Ø© ÙÙ‚Ø·: Ø§Ù„ØªØºØ±ÙŠØ¯Ø© | Ø§Ù„Ù…Ø´Ø§Ø¹Ø± | Ø§Ù„Ù„Ù‡Ø¬Ø©)
"""

# ==============================
# Google Drive
# ==============================
drive.mount('/content/drive')

# ==============================
# OpenAI API (Batch)
# ==============================
OPENAI_FILES_URL = "https://api.openai.com/v1/files"
OPENAI_BATCHES_URL = "https://api.openai.com/v1/batches"
OPENAI_CHAT_ENDPOINT = "/v1/chat/completions"
MODEL = "gpt-4o"
BATCH_COMPLETION_WINDOW = "24h"

# ==============================
# API KEY
# ==============================
API_KEY = ""
try:
    API_KEY = userdata.get('Mohanad') or userdata.get('OPENAI_API_KEY') or ""
except Exception:
    pass
if not API_KEY:
    API_KEY = os.environ.get('Mohanad', '') or os.environ.get('OPENAI_API_KEY', '')
if not API_KEY:
    print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ OpenAI (Mohanad / OPENAI_API_KEY)")
    raise SystemExit(1)

# ==============================
# Ø¨Ù†ÙˆÙƒ Ø§Ù„ØªÙ†ÙˆÙŠØ¹ Ù„Ù„Ø´Ø§Ù…
# ==============================
DIALECTS = ["Ø³ÙˆØ±ÙŠØ©", "Ù„Ø¨Ù†Ø§Ù†ÙŠØ©", "Ø£Ø±Ø¯Ù†ÙŠØ©", "ÙÙ„Ø³Ø·ÙŠÙ†ÙŠØ©"]

DIALECT_HINTS = {
    "Ø³ÙˆØ±ÙŠØ©":   "Ù…Ù…ÙƒÙ†: Ø´Ùˆ/Ù„Ø³Ù‘Ø§/Ù‡Ù„Ù‘Ù‚/ÙƒØªÙŠØ±/ØªÙ…Ø§Ù…/ÙŠØ¹Ù†ÙŠ/Ù…Ùˆ/Ø¨Ø¯Ù‘ÙŠ/Ù…Ø´Ø§Ù† â€” Ø¨Ù„Ø§ Ù…Ø¨Ø§Ù„ØºØ©.",
    "Ù„Ø¨Ù†Ø§Ù†ÙŠØ©": "Ù…Ù…ÙƒÙ†: Ø´Ùˆ/Ù…Ø²Ø¨ÙˆØ·/Ø¹Ù†Ø¬Ø¯/Ù‡ÙŠÙƒ/Ù‚Ø¯Ù‘ÙŠ/ÙƒØªÙŠØ±/Ù‡Ù„Ù‚/Ù…Ø§ Ø¨Ø¹Ø±Ù/Ø®Ù„Øµ â€” Ø¨Ù„Ø§ Ù…Ø¨Ø§Ù„ØºØ©.",
    "Ø£Ø±Ø¯Ù†ÙŠØ©":  "Ù…Ù…ÙƒÙ†: Ø´Ùˆ/Ù„Ø³Ù‘Ù‡/Ù‡Ø³Ù‡/Ù‚Ø¯Ù‘ÙŠØ´/Ù„ÙŠØ´/Ù…Ø²Ø¨ÙˆØ·/Ø²Ù„Ù…Ø©/Ù‚Ø¹Ø¯Ø©/ØªÙ…Ø§Ù… â€” Ø¨Ù„Ø§ Ù…Ø¨Ø§Ù„ØºØ©.",
    "ÙÙ„Ø³Ø·ÙŠÙ†ÙŠØ©":"Ù…Ù…ÙƒÙ†: Ø´Ùˆ/Ù„Ø³Ù‘Ø§/Ù‡Ù„Ù‘Ù‚/Ø²Ù„Ù…Ø©/Ø¹Ù†Ø¬Ø¯/Ù…Ù†ÙŠØ­/Ù‚Ø¯Ù‘ÙŠØ´/Ø·Ø¨Ø¹Ù‹Ø§/Ø®Ù„Øµ â€” Ø¨Ù„Ø§ Ù…Ø¨Ø§Ù„ØºØ©."
}

TOPICS = [
    "Ø§Ù„Ø¹Ø¬Ù‚Ø© ÙˆØ§Ù„Ù…ÙˆØ§ØµÙ„Ø§Øª", "Ø§Ù„Ø³Ø±ÙÙŠØ³/Ø§Ù„Ù…ÙŠÙƒØ±Ùˆ ÙˆØ§Ù„ØªØ¨Ø¯ÙŠÙ„", "Ø§Ù„Ù‚Ù‡ÙˆØ© Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ù„ÙƒÙˆÙ†/Ø§Ù„ØªÙŠØ±Ø§Ø³", "Ø§Ù„Ù…Ø´Ø§ÙˆÙŠØ± ÙˆØ§Ù„Ø·Ù„Ø¹Ø§Øª",
    "Ø§Ù„Ù…Ù†Ø§Ù‚ÙŠØ´ ÙˆØ§Ù„ÙÙ„Ø§ÙÙ„ ÙˆØ§Ù„Ø´Ø§ÙˆØ±Ù…Ø§", "Ø§Ù„Ù…Ù‚Ù„ÙˆØ¨Ø© ÙˆØ§Ù„Ù…Ù†Ø³Ù ÙˆØ§Ù„Ø£ÙƒÙ„Ø§Øª Ø§Ù„Ø¨ÙŠØªÙŠØ©", "Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© ÙˆØ§Ù„Ù…Ø­Ø§Ø¶Ø±Ø§Øª", "Ø§Ù„Ø´ØºÙ„ ÙˆØ§Ù„Ø¯ÙˆØ§Ù…",
    "Ø§Ù„Ù…ÙˆÙ„Ø§Øª ÙˆØ§Ù„Ø£Ø³ÙˆØ§Ù‚", "Ø§Ù„Ù…Ø¨Ø§Ø±ÙŠØ§Øª ÙˆØ§Ù„ÙƒØ±Ø©", "Ø§Ù„ØªØ³ÙˆÙ‚ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ", "Ø§Ù„Ø³Ù‡Ø±Ø© Ù…Ø¹ Ø§Ù„Ø£ØµØ­Ø§Ø¨",
    "Ø§Ù„Ø·Ù‚Ø³ (Ù…Ø·Ø±/Ø¨Ø±Ø¯/Ø´ÙˆØ¨)", "Ø§Ù„Ù…Ø´Ø§ÙˆÙŠØ± Ù„Ù„Ø¬Ø¨Ù„/Ø§Ù„Ø¶ÙŠØ¹Ø©", "Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª ÙˆØ§Ù„Ø³ÙŠÙ†ÙŠÙ…Ø§", "Ø§Ù„ØµØ¨Ø­ Ø¨Ø¯Ø±ÙŠ ÙˆØ§Ù„Ù†Ø´Ø§Ø·"
]

STYLES   = ["Ø³Ø§Ø®Ø±", "Ù„Ø·ÙŠÙ", "Ù…ØªÙØ§Ø¦Ù„", "Ù…Ø³ØªØ¹Ø¬Ù„", "Ù…Ù†Ø²Ø¹Ø¬", "Ø³Ø¤Ø§Ù„ Ù…Ø¨Ø§Ø´Ø±", "Ù†ÙˆØ³ØªØ§Ù„Ø¬ÙŠØ§", "ØªØ­ÙÙŠØ²ÙŠ", "Ø­ÙƒØ§Ø¦ÙŠ Ù‚ØµÙŠØ±"]
PERSONAS = ["Ø·Ø§Ù„Ø¨/Ù€Ø© Ø¬Ø§Ù…Ø¹Ø©", "Ù…ÙˆØ¸Ù/Ù€Ø© Ù…ÙƒØªØ¨", "Ø±ÙŠØ§Ø¶ÙŠ/Ù€Ø© Ø¬ÙŠÙ…", "Ù…Ø·ÙˆØ±/Ù€Ø© Ø¨Ø±Ù…Ø¬ÙŠØ§Øª", "Ø£Ø¨/Ø£Ù… Ø´Ø§Ø¨", "Ù…Ù‡ÙˆÙˆØ³/Ù€Ø© Ù‚Ù‡ÙˆØ©", "Ø´Ø®Øµ ÙŠØ­Ø¨ Ø§Ù„Ø³ÙØ±", "Ø¹Ø§Ø´Ù‚/Ù€Ø© Ø£ÙƒÙ„"]
TIMES    = ["Ø§Ù„ØµØ¨Ø­ Ø¨Ø¯Ø±ÙŠ", "Ù‚Ø¨Ù„ Ø§Ù„Ø¯ÙˆØ§Ù…", "Ø¨Ø¹Ø¯ Ø§Ù„Ø¯ÙˆØ§Ù…", "Ø¨Ø¹Ø¯ Ø§Ù„Ø¹Ø´Ø§Ø¡", "ÙˆÙ‚Øª Ø§Ù„ØºØ±ÙˆØ¨", "ÙˆÙ‚Øª Ø§Ù„Ø´ÙˆØ¨", "ÙˆÙ‚Øª Ø§Ù„Ù…Ø·Ø±"]

SCENARIOS = [
    "ÙˆØ§Ù‚Ù Ø¹Ø§Ù„Ø¥Ø´Ø§Ø±Ø© ÙˆØ§Ù„Ø²Ø­Ù…Ø© Ø®Ø§Ù†Ù‚Ø©", 
    "Ø±Ø§ÙƒØ¨ Ø³Ø±ÙÙŠØ³ ÙˆØ¹Ù… ÙŠØ³ØªÙ†Ù‰ ÙŠÙØ¶Ù‰ ÙƒØ±Ø³ÙŠ", 
    "Ù‚Ø¹Ø¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ù„ÙƒÙˆÙ† Ù…Ø¹ ÙÙ†Ø¬Ø§Ù† Ù‚Ù‡ÙˆØ©", 
    "Ø·Ù„Ø¹Ø© Ø®ÙÙŠÙØ© Ø¹Ø§Ù„Ø¬Ø¨Ù„/Ù…Ø²Ø±Ø¹Ø©", 
    "ÙˆØ§Ù‚Ù Ø¨Ø§Ù„Ø¯ÙˆØ± Ø¹ÙØ±Ù† Ù…Ù†Ø§Ù‚ÙŠØ´",
    "Ø¯Ø§Ø®Ù„ Ù…ÙˆÙ„ ÙˆÙŠØ¯ÙˆÙ‘Ø± Ø¹Ù„Ù‰ Ø®ØµÙ…",
    "Ø±Ø§Ø¬Ø¹ Ù…Ù† Ø§Ù„Ø¯ÙˆØ§Ù… ÙˆØªØ¹Ø¨Ø§Ù†",
    "ÙŠØ­Ø¶Ø± Ù„Ù…Ø¨Ø§Ø±Ø§Ø© Ù…Ø¹ Ø§Ù„Ø´Ø¨Ø§Ø¨",
    "ÙŠØ³ØªÙ†Ù‰ Ø·Ù„Ø¨ Ø¯Ù„ÙŠÙØ±ÙŠ",
    "ÙŠÙˆØ«Ù‘Ù‚ Ù„Ø­Ø¸Ø© ØºØ±ÙˆØ¨ Ø­Ù„ÙˆØ©",
    "Ù…Ø±ØªØ¨ Ø³Ù‡Ø±Ø© Ø¨Ø³ÙŠØ·Ø© Ø¨Ø§Ù„Ø¨ÙŠØª"
]

recent_topics = deque(maxlen=24)

# ==============================
# Sentiment Utilities
# ==============================
SENTIMENT_LABELS = ["Ø¥ÙŠØ¬Ø§Ø¨ÙŠ", "Ø³Ù„Ø¨ÙŠ", "Ù…Ø­Ø§ÙŠØ¯"]

def normalize_sentiment_label(v: str) -> str:
    if not v:
        return "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
    v = str(v).strip().lower()
    m = {
        'positive': 'Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'pos': 'Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'Ø§ÙŠØ¬Ø§Ø¨ÙŠ': 'Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'Ø¥ÙŠØ¬Ø§Ø¨ÙŠ': 'Ø¥ÙŠØ¬Ø§Ø¨ÙŠ',
        'negative': 'Ø³Ù„Ø¨ÙŠ', 'neg': 'Ø³Ù„Ø¨ÙŠ', 'Ø³Ù„Ø¨ÙŠ': 'Ø³Ù„Ø¨ÙŠ',
        'neutral': 'Ù…Ø­Ø§ÙŠØ¯', 'neu': 'Ù…Ø­Ø§ÙŠØ¯', 'Ù…Ø­Ø§ÙŠØ¯': 'Ù…Ø­Ø§ÙŠØ¯'
    }
    return m.get(v, 'Ù…Ø­Ø§ÙŠØ¯')

def choose_non_repeat(options, memory: deque):
    pool = [o for o in options if o not in memory]
    if not pool:
        memory.clear()
        pool = options[:]
    pick = random.choice(pool)
    memory.append(pick)
    return pick

def build_context():
    topic   = choose_non_repeat(TOPICS, recent_topics)
    style   = random.choice(STYLES)
    persona = random.choice(PERSONAS)
    time_s  = random.choice(TIMES)
    scen    = random.choice(SCENARIOS)
    dialect = random.choice(DIALECTS)

    flags = {
        'hashtag': random.random() < 0.30,
        'emoji':   random.random() < 0.25,
        'english': random.random() < 0.20,
        'franco':  random.random() < 0.10,
    }

    return {
        'dialect': dialect,
        'topic': topic,
        'style': style,
        'persona': persona,
        'time': time_s,
        'city': "",
        'scenario': scen,
        'flags': flags,
        'dialect_hint': DIALECT_HINTS.get(dialect, "")
    }

def build_user_message(ctx: dict, target_sent: str) -> str:
    instr = (
        "Ø§ÙƒØªØ¨ ØªØºØ±ÙŠØ¯Ø© ÙˆØ§Ø­Ø¯Ø© Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø´Ø§Ù…ÙŠØ© ({dialect}) ÙÙ‚Ø·.\n"
        "Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©: {sentiment}. ÙŠØ¬Ø¨ Ø£Ù† ØªÙØ¹Ø¨Ù‘Ø± Ø§Ù„ØªØºØ±ÙŠØ¯Ø© Ø¨ÙˆØ¶ÙˆØ­ Ø¹Ù† Ù‡Ø°Ø§ Ø§Ù„Ø´Ø¹ÙˆØ±.\n"
        "Ø£Ø¹ÙØ¯ JSON ØµØ§Ù„Ø­ ÙÙ‚Ø· Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø´ÙƒÙ„: {{\"text\": \"...\", \"sentiment\": \"{sentiment}\"}}.\n"
        "Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯: Ø·ÙˆÙ„ 60â€“200 Ø­Ø±Ù ØªÙ‚Ø±ÙŠØ¨Ù‹Ø§ØŒ Ù„Ù‡Ø¬Ø© Ø´Ø§Ù…ÙŠØ© Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆÙˆØ§Ø¶Ø­Ø©ØŒ Ø¨Ø¯ÙˆÙ† Ø±ÙˆØ§Ø¨Ø· Ø£Ùˆ Ø£Ø³Ù…Ø§Ø¡/Ø£Ø±Ù‚Ø§Ù…/Ù…Ù†Ø´Ù†Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ©ØŒ "
        "Ø¹Ù„Ø§Ù…Ø§Øª ØªØ±Ù‚ÙŠÙ… Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙˆÙ…Ù…Ù†ÙˆØ¹ Ø°ÙƒØ± Ø£ÙŠ Ø£Ø³Ù…Ø§Ø¡ Ù…Ø¯Ù†/Ù…Ù†Ø§Ø·Ù‚/Ø¯ÙˆÙ„ Ù†Ù‡Ø§Ø¦ÙŠÙ‹Ø§.\n"
        "ØªÙ„Ù…ÙŠØ­Ø§Øª Ø£Ø³Ù„ÙˆØ¨ÙŠØ© Ø§Ø®ØªÙŠØ§Ø±ÙŠØ©: {dialect_hint}\n"
        "Ø·Ø¨Ù‘Ù‚ Ø§Ù„Ø£Ø¹Ù„Ø§Ù… Ø§Ù„ØªØ§Ù„ÙŠØ© Ø­Ø±ÙÙŠÙ‹Ø§: Ø§Ù„Ù‡Ø§Ø´ØªØ§Ø¬: {hashflag} â€” Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠ: {emoflag} â€” Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ: {engflag} â€” ÙØ±Ø§Ù†ÙƒÙˆ: {fflag}.\n"
        "Ø§Ù„Ø³ÙŠØ§Ù‚: Ù…ÙˆØ¶ÙˆØ¹={topic}ØŒ Ø£Ø³Ù„ÙˆØ¨={style}ØŒ Ø´Ø®ØµÙŠØ© Ù…ØªÙƒÙ„Ù…Ø©={persona}ØŒ ØªÙˆÙ‚ÙŠØª={time}ØŒ (Ø¨Ø¯ÙˆÙ† Ø°ÙƒØ± Ù…Ø¯Ù†/Ù…Ù†Ø§Ø·Ù‚/Ø¯ÙˆÙ„)ØŒ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ={scenario}.\n"
        "Ø£Ø¹ÙØ¯ JSON ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ø´Ø±Ø­ Ø¥Ø¶Ø§ÙÙŠØŒ ÙˆØ¨Ø§Ù„Ø­Ù‚ÙˆÙ„: text, sentiment Ù„Ø§ ØºÙŠØ±."
    ).format(
        dialect=ctx['dialect'], sentiment=target_sent, dialect_hint=ctx.get('dialect_hint', ''),
        topic=ctx['topic'], style=ctx['style'], persona=ctx['persona'], time=ctx['time'],
        scenario=ctx['scenario'],
        hashflag=("Ø£Ø¶ÙÙ Ù‡Ø§Ø´ØªØ§Ø¬ ÙˆØ§Ø­Ø¯ Ù…Ù†Ø§Ø³Ø¨" if ctx['flags']['hashtag'] else "Ù…Ù…Ù†ÙˆØ¹ Ø§Ù„Ù‡Ø§Ø´ØªØ§Ø¬"),
        emoflag=("Ù…Ø³Ù…ÙˆØ­ Ø¨Ø¥ÙŠÙ…ÙˆØ¬ÙŠ ÙˆØ§Ø­Ø¯ Ø¨Ø­Ø¯ Ø£Ù‚ØµÙ‰ Ø§Ø«Ù†ÙŠÙ†" if ctx['flags']['emoji'] else "Ù…Ù…Ù†ÙˆØ¹ Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠ"),
        engflag=("Ø£Ø¯Ø®Ù„ ÙƒÙ„Ù…Ø© Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ÙˆØ§Ø­Ø¯Ø© Ù…Ù†Ø§Ø³Ø¨Ø©" if ctx['flags']['english'] else "Ù…Ù…Ù†ÙˆØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©"),
        fflag=("Ù…Ø³Ù…ÙˆØ­ Ø¨ÙƒÙ„Ù…Ø© ÙØ±Ø§Ù†ÙƒÙˆ ÙˆØ§Ø­Ø¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£ÙƒØ«Ø±" if ctx['flags']['franco'] else "Ù…Ù…Ù†ÙˆØ¹ Ø§Ù„ÙØ±Ø§Ù†ÙƒÙˆ")
    )
    return instr

# ==============================
# Allocation of target sentiments
# ==============================
def normalize_weights(weights: dict) -> dict:
    w = {lbl: float(max(0.0, weights.get(lbl, 0.0))) for lbl in SENTIMENT_LABELS}
    s = sum(w.values())
    if s <= 0:
        n = len(SENTIMENT_LABELS)
        return {lbl: 1.0 / n for lbl in SENTIMENT_LABELS}
    return {k: v / s for k, v in w.items()}

def build_sentiment_plan(num_items: int, weights: dict, mode: str = 'deterministic') -> list:
    weights = normalize_weights(weights)
    labels = SENTIMENT_LABELS
    if mode not in ('deterministic', 'probabilistic'):
        mode = 'deterministic'
    if mode == 'probabilistic':
        probs = [weights[lbl] for lbl in labels]
        return random.choices(labels, weights=probs, k=num_items)

    raw_counts = {lbl: num_items * weights[lbl] for lbl in labels}
    floor_counts = {lbl: int(math.floor(raw_counts[lbl])) for lbl in labels}
    assigned = sum(floor_counts.values())
    remainder = num_items - assigned
    fracs = sorted(((lbl, raw_counts[lbl] - floor_counts[lbl]) for lbl in labels), key=lambda x: x[1], reverse=True)
    for i in range(remainder):
        floor_counts[fracs[i % len(fracs)][0]] += 1
    plan = []
    for lbl in labels:
        plan.extend([lbl] * floor_counts[lbl])
    random.shuffle(plan)
    return plan[:num_items]

# ==============================
# Ù…Ø§Ù†Ø¹ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø¯Ù†/Ø§Ù„Ø¯ÙˆÙ„
# ==============================
CITY_NAME_BLACKLIST = [
    # Ø³ÙˆØ±ÙŠØ§
    "Ø¯Ù…Ø´Ù‚","Ø§Ù„Ø´Ø§Ù…","Ø­Ù„Ø¨","Ø­Ù…Øµ","Ø­Ù…Ø§Ø©","Ø§Ù„Ù„Ø§Ø°Ù‚ÙŠØ©","Ø·Ø±Ø·ÙˆØ³","Ø¯Ø±Ø¹Ø§","Ø§Ù„Ø³ÙˆÙŠØ¯Ø§Ø¡","Ø¥Ø¯Ù„Ø¨","Ø§Ù„Ø±Ù‚Ø©","Ø¯ÙŠØ± Ø§Ù„Ø²ÙˆØ±","Ø§Ù„Ø­Ø³ÙƒØ©","Ø§Ù„Ù‚Ø§Ù…Ø´Ù„ÙŠ",
    # Ù„Ø¨Ù†Ø§Ù†
    "Ø¨ÙŠØ±ÙˆØª","Ø·Ø±Ø§Ø¨Ù„Ø³","ØµÙŠØ¯Ø§","ØµÙˆØ±","Ø²Ø­Ù„Ø©","Ø¨Ø¹Ù„Ø¨Ùƒ","Ø¬Ø¨ÙŠÙ„","Ø¬ÙˆÙ†ÙŠØ©","Ø§Ù„Ù†Ø¨Ø·ÙŠØ©",
    # Ø§Ù„Ø£Ø±Ø¯Ù†
    "Ø¹Ù…Ù‘Ø§Ù†","Ø¹Ù…Ø§Ù†","Ø§Ù„Ø²Ø±Ù‚Ø§Ø¡","Ø¥Ø±Ø¨Ø¯","Ø§Ù„Ø¹Ù‚Ø¨Ø©","Ù…Ø§Ø¯Ø¨Ø§","Ø§Ù„Ø³Ù„Ø·","Ø¬Ø±Ø´","Ø¹Ø¬Ù„ÙˆÙ†","Ø§Ù„ÙƒØ±Ùƒ","Ù…Ø¹Ø§Ù†","Ø§Ù„Ø·ÙÙŠÙ„Ø©",
    # ÙÙ„Ø³Ø·ÙŠÙ†
    "Ø§Ù„Ù‚Ø¯Ø³","Ø±Ø§Ù… Ø§Ù„Ù„Ù‡","ØºØ²Ø©","Ù†Ø§Ø¨Ù„Ø³","Ø§Ù„Ø®Ù„ÙŠÙ„","Ø¨ÙŠØª Ù„Ø­Ù…","Ø·ÙˆÙ„ÙƒØ±Ù…","Ø¬Ù†ÙŠÙ†","Ø±ÙØ­","Ø®Ø§Ù† ÙŠÙˆÙ†Ø³","Ù‚Ù„Ù‚ÙŠÙ„ÙŠØ©",
    # Ø¯ÙˆÙ„
    "Ø³ÙˆØ±ÙŠØ§","Ù„Ø¨Ù†Ø§Ù†","Ø§Ù„Ø£Ø±Ø¯Ù†","Ø§Ù„Ø§Ø±Ø¯Ù†","ÙÙ„Ø³Ø·ÙŠÙ†"
]
DELIMS = r"[\s\.,!?Ø›ØŒ:\-â€“â€”â€¦\(\)\[\]\"'Â«Â»]"
GEO_PATTERN = re.compile(
    r"(^|" + DELIMS + r")(" + "|".join(map(re.escape, CITY_NAME_BLACKLIST)) + r")(?=($|" + DELIMS + r"))",
    flags=re.IGNORECASE
)

def _clean_spaces_punct(s: str) -> str:
    s = re.sub(r"\s{2,}", " ", s)
    s = re.sub(r"\s*([ØŒ,\.!?â€¦Ø›:])\s*", r" \1 ", s)
    return s.strip()

def remove_geo_names(text: str) -> str:
    if text is None:
        return text
    s = str(text)
    s2 = GEO_PATTERN.sub(lambda m: m.group(1), s)
    if s2 != s:
        s2 = _clean_spaces_punct(s2)
    return s2

def normalize_text(s: str) -> str:
    s = s.strip()
    if s.startswith('"') and s.endswith('"'):
        s = s[1:-1]
    s = re.sub(r"\s+", " ", s)
    s = re.sub(r"https?://\S+", "", s)
    s = re.sub(r"@[A-Za-z0-9_]+", "", s)
    s = remove_geo_names(s)
    return s.strip()

# ==============================
# JSONL / Mapping
# ==============================
def prepare_jsonl_for_batch(num_items: int, jsonl_path: str, mapping_path: str,
                            sentiment_weights: dict, allocation_mode: str = 'deterministic'):
    plan = build_sentiment_plan(num_items, sentiment_weights, allocation_mode)

    from collections import Counter
    cnt = Counter(plan)
    total = sum(cnt.values()) or 1
    print("ğŸ“ Ø®Ø·Ø© Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„:")
    for lbl in SENTIMENT_LABELS:
        print(f"  - {lbl}: {cnt.get(lbl, 0)} ({(cnt.get(lbl, 0)/total*100):.2f}%)")

    with open(jsonl_path, 'w', encoding='utf-8') as jf, open(mapping_path, 'w', encoding='utf-8') as mf:
        ts = int(time.time() * 1000)
        for i in range(num_items):
            ctx = build_context()
            target_sent = plan[i]
            custom_id = f"tw-levant-sent-{ts}-{i+1}"
            user_msg = build_user_message(ctx, target_sent)
            body = {
                "custom_id": custom_id,
                "method": "POST",
                "url": OPENAI_CHAT_ENDPOINT,
                "body": {
                    "model": MODEL,
                    "messages": [
                        {"role": "system", "content": "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ ÙŠÙƒØªØ¨ ØªØºØ±ÙŠØ¯Ø§Øª Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø§Øª Ø§Ù„Ø´Ø§Ù…ÙŠØ© Ø¨Ø¯Ù‚Ø© ÙˆØ¨Ø¯ÙˆÙ† Ø´Ø±Ø­."},
                        {"role": "user",   "content": user_msg}
                    ],
                    "response_format": {"type": "json_object"},
                    "temperature": 0.95,
                    "top_p": 0.95,
                    "max_tokens": 220
                }
            }
            jf.write(json.dumps(body, ensure_ascii=False) + "\n")
            mf.write(json.dumps({"custom_id": custom_id, "ctx": ctx, "target_sentiment": target_sent}, ensure_ascii=False) + "\n")
    print(f"ğŸ—‚ï¸ JSONL: {jsonl_path}")
    print(f"ğŸ—ºï¸ Mapping: {mapping_path}")

# ==============================
# Batch API helpers
# ==============================
def upload_file_for_batch(jsonl_path: str) -> str:
    headers = {"Authorization": f"Bearer {API_KEY}"}
    files = {"file": (os.path.basename(jsonl_path), open(jsonl_path, 'rb'))}
    data = {"purpose": "batch"}
    r = requests.post(OPENAI_FILES_URL, headers=headers, files=files, data=data, timeout=120)
    if r.status_code != 200:
        raise RuntimeError(f"File upload failed: {r.status_code} {r.text}")
    file_id = r.json()["id"]
    print(f"ğŸ“ ØªÙ… Ø§Ù„Ø±ÙØ¹: file_id={file_id}")
    return file_id

def create_batch_job(file_id: str, completion_window: str = BATCH_COMPLETION_WINDOW) -> str:
    headers = {"Authorization": f"Bearer {API_KEY}", "Content-Type": "application/json"}
    payload = {
        "input_file_id": file_id,
        "endpoint": OPENAI_CHAT_ENDPOINT,
        "completion_window": completion_window
    }
    r = requests.post(OPENAI_BATCHES_URL, headers=headers, json=payload, timeout=60)
    if r.status_code != 200:
        raise RuntimeError(f"Create batch failed: {r.status_code} {r.text}")
    batch_id = r.json()["id"]
    print(f"ğŸ“¦ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Batch: batch_id={batch_id}")
    return batch_id

def poll_batch_until_done(batch_id: str, poll_interval: int = 20) -> dict:
    headers = {"Authorization": f"Bearer {API_KEY}"}
    while True:
        r = requests.get(f"{OPENAI_BATCHES_URL}/{batch_id}", headers=headers, timeout=60)
        if r.status_code != 200:
            raise RuntimeError(f"Batch status failed: {r.status_code} {r.text}")
        obj = r.json()
        status = obj.get("status")
        print(f"â³ Ø­Ø§Ù„Ø© Ø§Ù„Ø¨Ø§ØªØ´: {status}")
        if status in ("completed", "failed", "canceled"):
            return obj
        time.sleep(poll_interval)

def download_file_content(file_id: str) -> str:
    headers = {"Authorization": f"Bearer {API_KEY}"}
    r = requests.get(f"{OPENAI_FILES_URL}/{file_id}/content", headers=headers, timeout=120)
    if r.status_code != 200:
        raise RuntimeError(f"Download file failed: {r.status_code} {r.text}")
    return r.text

# ==============================
# Mapping loader
# ==============================
def load_mapping_jsonl(mapping_path: str) -> dict:
    mapping = {}
    with open(mapping_path, 'r', encoding='utf-8') as f:
        for line in f:
            if not line.strip():
                continue
            row = json.loads(line)
            mapping[row.get('custom_id')] = row
    return mapping

# ==============================
# Parse batch output  -> (Ù†Ø®Ø²Ù‘Ù† ÙÙ‚Ø· Ù…Ø§ Ù†Ø­ØªØ§Ø¬Ù‡)
# ==============================
def parse_batch_output(jsonl_text: str, mapping: dict) -> list:
    tweets = []
    for line in jsonl_text.splitlines():
        if not line.strip():
            continue
        obj = json.loads(line)

        if obj.get("error"):
            continue

        resp = obj.get("response") or {}
        if resp.get("status_code") != 200:
            continue

        body = resp.get("body") or {}
        choices = body.get("choices") or []
        if not choices:
            continue

        content = choices[0].get("message", {}).get("content", "").strip()
        try:
            content_obj = json.loads(content) if isinstance(content, str) else content
        except json.JSONDecodeError:
            continue

        text = normalize_text(str(content_obj.get("text", "")).strip())
        sentiment_raw = content_obj.get("sentiment", "")
        sentiment = normalize_sentiment_label(sentiment_raw)
        if not text:
            continue

        custom_id = obj.get("custom_id")
        meta = mapping.get(custom_id, {}) if custom_id else {}
        ctx = meta.get('ctx', {}) if meta else {}

        tweets.append({
            'Ø§Ù„ØªØºØ±ÙŠØ¯Ø©': text,
            'Ø§Ù„Ù…Ø´Ø§Ø¹Ø±': sentiment if sentiment in SENTIMENT_LABELS else "Ù…Ø­Ø§ÙŠØ¯",
            'Ø§Ù„Ù„Ù‡Ø¬Ø©': ctx.get('dialect', '')
        })

    # ØªØ±Ù‚ÙŠÙ… Ø¯Ø§Ø®Ù„ÙŠ Ù„Ùˆ Ø§Ø­ØªØ¬ØªÙ‡ Ù„Ø§Ø­Ù‚Ù‹Ø§ (ØºÙŠØ± Ù…ÙƒØªÙˆØ¨ Ù„Ù„Ù…Ù„Ù)
    for i, row in enumerate(tweets, start=1):
        row['_idx'] = i

    return tweets

# ==============================
# Save to Excel (3 Ø£Ø¹Ù…Ø¯Ø© ÙÙ‚Ø·)
# ==============================
def save_to_excel(tweets: list, filename: str = "levant_tweets_MINIMAL_3cols.xlsx"):
    if not tweets:
        print("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ ØªØºØ±ÙŠØ¯Ø§Øª Ù„Ù„Ø­ÙØ¸!")
        return
    df = pd.DataFrame(tweets)
    # Ø«Ù„Ø§Ø«Ø© Ø£Ø¹Ù…Ø¯Ø© ÙÙ‚Ø· ÙˆØ¨Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
    df = df[['Ø§Ù„ØªØºØ±ÙŠØ¯Ø©', 'Ø§Ù„Ù…Ø´Ø§Ø¹Ø±', 'Ø§Ù„Ù„Ù‡Ø¬Ø©']].copy()

    file_path = f"/content/drive/MyDrive/{filename}"
    try:
        with pd.ExcelWriter(file_path, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='Ø§Ù„ØªØºØ±ÙŠØ¯Ø§Øª', index=False)
        print(f"âœ… ØªÙ… Ø­ÙØ¸ {len(df)} ØªØºØ±ÙŠØ¯Ø© ÙÙŠ: {file_path}")
        print("ğŸ“„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©: Ø§Ù„ØªØºØ±ÙŠØ¯Ø© | Ø§Ù„Ù…Ø´Ø§Ø¹Ø± | Ø§Ù„Ù„Ù‡Ø¬Ø©")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­ÙØ¸: {e}")

# ==============================
# Main
# ==============================
def main():
    print("ğŸš€ Ù…ÙˆÙ„Ø¯ ØªØºØ±ÙŠØ¯Ø§Øª Ø´Ø§Ù…ÙŠØ© â€” GPT-4o â€” Batch API â€” (Excel Ø¨Ø«Ù„Ø§Ø«Ø© Ø£Ø¹Ù…Ø¯Ø© ÙÙ‚Ø·)")
    print("=" * 100)

    # ---- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ----
    NUM_TWEETS = 100
    SENTIMENT_WEIGHTS = {'Ø¥ÙŠØ¬Ø§Ø¨ÙŠ': 0.3, 'Ø³Ù„Ø¨ÙŠ': 0.4, 'Ù…Ø­Ø§ÙŠØ¯': 0.3}
    ALLOCATION_MODE = 'deterministic'

    JSONL_PATH = "/content/drive/MyDrive/levant_MIN_requests.jsonl"
    MAPPING_PATH = "/content/drive/MyDrive/levant_MIN_mapping.jsonl"
    RAW_OUTPUT_JSONL_PATH = "/content/drive/MyDrive/levant_MIN_output.jsonl"

    # 1) ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø·Ù„Ø¨Ø§Øª
    prepare_jsonl_for_batch(
        num_items=NUM_TWEETS,
        jsonl_path=JSONL_PATH,
        mapping_path=MAPPING_PATH,
        sentiment_weights=SENTIMENT_WEIGHTS,
        allocation_mode=ALLOCATION_MODE,
    )

    # 2) Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù ÙˆØ¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¨Ø§ØªØ´
    file_id = upload_file_for_batch(JSONL_PATH)
    batch_id = create_batch_job(file_id, completion_window=BATCH_COMPLETION_WINDOW)

    # 3) Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±
    status_obj = poll_batch_until_done(batch_id, poll_interval=20)
    final_status = status_obj.get("status")
    print(f"âœ… Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: {final_status}")
    if final_status != "completed":
        print("âŒ Ù„Ù… ÙŠÙƒØªÙ…Ù„ Ø§Ù„Ø¨Ø§ØªØ´ Ø¨Ù†Ø¬Ø§Ø­. Ø§Ù„ØªÙØ§ØµÙŠÙ„:", status_obj)
        return

    # 4) ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
    output_file_id = status_obj.get("output_file_id")
    if not output_file_id:
        print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ output_file_id.")
        return

    output_jsonl = download_file_content(output_file_id)
    with open(RAW_OUTPUT_JSONL_PATH, 'w', encoding='utf-8') as f:
        f.write(output_jsonl)
    print(f"ğŸ“¥ Ø­ÙØ¸ Ø§Ù„Ø®Ø§Ù… ÙÙŠ: {RAW_OUTPUT_JSONL_PATH}")

    # 5) ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ -> Ø«Ù„Ø§Ø«Ø© Ø£Ø¹Ù…Ø¯Ø© ÙÙ‚Ø·
    mapping = load_mapping_jsonl(MAPPING_PATH)
    tweets = parse_batch_output(output_jsonl, mapping)
    if not tweets:
        print("âŒ Ù„Ù… ØªÙØ³ØªØ®Ø±Ø¬ ØªØºØ±ÙŠØ¯Ø§Øª Ù…Ù† Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø§ØªØ´.")
        return

    # 6) Ø­ÙØ¸ Excel (3 Ø£Ø¹Ù…Ø¯Ø© ÙÙ‚Ø·)
    save_to_excel(tweets, filename="levant_tweets_MINIMAL_3cols.xlsx")

# ==============================
# Entrypoint
# ==============================
if __name__ == "__main__":
    try:
        import openpyxl  # noqa: F401
    except ImportError:
        print("ğŸ“¦ ØªØ«Ø¨ÙŠØª openpyxl...")
        import sys
        # ÙŠØ¹Ù…Ù„ Ø¯Ø§Ø®Ù„ Ø®Ù„ÙŠØ© ÙƒÙˆÙ„Ø§Ø¨
        !pip install openpyxl -q
        import openpyxl  # noqa: F401

    main()
